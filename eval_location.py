import json
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Union

import mediapy as media
import torch
import tyro
from rich import box, style
from rich.panel import Panel
from rich.progress import BarColumn, Progress, TaskProgressColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.table import Table
from rich.console import Group
from rich.panel import Panel
from rich.live import Live
from typing_extensions import Annotated

from nerfstudio.cameras.cameras import Cameras, CameraType
from nerfstudio.utils import colormaps, install_checks
from nerfstudio.utils.eval_utils import eval_setup
from nerfstudio.utils.rich_utils import CONSOLE
from nerfstudio.viewer_legacy.server.utils import three_js_perspective_camera_focal_length

def load_keyframes_from_json(camera_path: Dict[str, Any]) -> Cameras:
    """Takes a camera path dictionary and returns a trajectory as a Camera instance.

    Args:
        camera_path: A dictionary of the camera path information coming from the viewer.

    Returns:
        A Cameras instance with the camera path.
    """

    image_height = camera_path["render_height"]
    image_width = camera_path["render_width"]

    camera_type = CameraType.PERSPECTIVE

    c2ws = []
    fxs = []
    fys = []
    for camera in camera_path["keyframes"]:
        # pose
        c2w = torch.tensor(camera["matrix"]).view(4, 4)[:3]
        c2ws.append(c2w)

        # field of view
        fov = camera["fov"]
        focal_length = three_js_perspective_camera_focal_length(fov, image_height)
        fxs.append(focal_length)
        fys.append(focal_length)

    camera_to_worlds = torch.stack(c2ws, dim=0)
    fx = torch.tensor(fxs)
    fy = torch.tensor(fys)

    return Cameras(
        fx=fx,
        fy=fy,
        cx=image_width / 2,
        cy=image_height / 2,
        camera_to_worlds=camera_to_worlds,
        camera_type=camera_type,
    )

def load_queries_from_json(query_path: Dict[str, Any], rendered_widths, rendered_heights):
    image_height = query_path["imageHeight"]
    image_width = query_path["imageWidth"]

    scale_width = rendered_widths[0].item() / image_width
    scale_height = rendered_heights[0].item() / image_height
    queriesAll = []
    numQueries = 0
    for frame in query_path["keyframes"]:
        queries = {}
        for shape in frame["shapes"]:
            #align coordinates of box to rendered size
            points = shape["points"]
            points[0][0] *=scale_width
            points[1][0] *=scale_width
            points[0][1] *=scale_height
            points[1][1] *=scale_height
            if shape["label"] in queries:
                queries[shape["label"]].append(points)
            else:
                queries[shape["label"]] = [points]
                numQueries += 1
        queriesAll.append(queries)

    return queriesAll, numQueries


@dataclass
class ComputeLocalization:
    """Render a camera path generated by the viewer or blender add-on."""

    load_config: Path
    """Path to config YAML file."""
    camera_path_filename: Path = Path("camera_path.json")
    """Filename of the camera transforms."""
    query_path_filename: Path = Path("query_path.json")
    """Filename of the queries."""
    render_relevancy: bool = False
    """Specifies wether to render out relevancy maps for prompts"""

    def main(self) -> None:
        """Main function."""
        _, pipeline, _, _ = eval_setup(
            self.load_config,
            eval_num_rays_per_chunk=None,
            test_mode="inference",
        )

        if self.render_relevancy:
            install_checks.check_ffmpeg_installed()

            output_image_dir = self.camera_path_filename.parent / "relevancy_maps"
            output_image_dir.mkdir(parents=True, exist_ok=True)

        with open(self.camera_path_filename, "r", encoding="utf-8") as f:
            camera_path = json.load(f)
        cameras = load_keyframes_from_json(camera_path)

        #TODO: load queries and bounding boxes
        with open(self.query_path_filename, "r", encoding="utf-8") as f:
            query_path = json.load(f)
        
        queries, numQueries = load_queries_from_json(query_path, cameras.width, cameras.height)

        colormap_options = colormaps.ColormapOptions(colormap_min=-1.0, colormap="turbo")

        #cameras.rescale_output_resolution(1.0 / self.downscale_factor)
        cameras = cameras.to(pipeline.device)

        frameProgress = Progress(
            TextColumn("Keyframes"),
            BarColumn(),
            TaskProgressColumn(
                text_format="[progress.percentage]{task.completed}/{task.total:>.0f}({task.percentage:>3.1f}%)",
                show_speed=True,
            ),
            #ItersPerSecColumn(suffix="fps"),
            TimeRemainingColumn(elapsed_when_finished=False, compact=False),
            TimeElapsedColumn(),
        )
        labelProgress = Progress(
            TextColumn("Labels"),
            BarColumn(),
            TaskProgressColumn(
                text_format="[progress.percentage]{task.completed}/{task.total:>.0f}({task.percentage:>3.1f}%)",
                show_speed=True,
            ),
            TimeRemainingColumn(elapsed_when_finished=False, compact=False)
        )

        group = Group(frameProgress,labelProgress)
        
        live = Live(group)

        rendered_output_name = "relevancy_0"
        successfulLocalizations = 0
        with live:
            for camera_idx in frameProgress.track(range(cameras.size), description=""):

                labels = queries[camera_idx].keys()

                id = labelProgress.add_task(description="")
                for label in labelProgress.track(labels,task_id=id, description=""):
                    #Set lerf prompt to query
                    pipeline.image_encoder.set_positives([label])

                    with torch.no_grad():
                        outputs = pipeline.model.get_outputs_for_camera(
                            cameras[camera_idx : camera_idx + 1], obb_box=None
                        )

                    if rendered_output_name not in outputs:
                        CONSOLE.rule("Error", style="red")
                        CONSOLE.print(f"Could not find {rendered_output_name} in the model outputs", justify="center")
                        CONSOLE.print(
                            f"Please use a lerf model config to run this evaluation script", justify="center"
                        )
                        sys.exit(1)
                    output_image = outputs[rendered_output_name]

                    #Check if maximal pixel of prompt is within bounding box
                    maxVal = (output_image==torch.max(output_image)).nonzero()[0][:2]

                    boxes = queries[camera_idx][label]
                    for points in boxes:
                        if points[0][0] < maxVal[1].item() and points[1][0] > maxVal[1].item() and points[0][1] < maxVal[0].item() and points[1][1] > maxVal[0].item():
                            successfulLocalizations +=1
                            break
                    
                    if self.render_relevancy:
                        output_image = (
                            colormaps.apply_colormap(
                                image=output_image,
                                colormap_options=colormap_options,
                            )
                            .cpu()
                            .numpy()
                        )

                        media.write_image(output_image_dir / f"{camera_idx:01d}_{label}.png", output_image, fmt="png")
                labelProgress.update(id,visible=False)
                    

        localizationScore = successfulLocalizations / numQueries
        table = Table(
            title=None,
            show_header=False,
            box=box.MINIMAL,
            title_style=style.Style(bold=True),
        )
        table.add_row("Localization Score:", str(localizationScore))
        CONSOLE.print(Panel(table, title="[bold][green]:tada: Done :tada:[/bold]", expand=False))

Commands = tyro.conf.FlagConversionOff[
    Union[
        Annotated[ComputeLocalization, tyro.conf.subcommand(name="localization")],
    ]
]


def entrypoint():
    """Entrypoint for use with pyproject scripts."""
    tyro.extras.set_accent_color("bright_yellow")
    tyro.cli(Commands).main()


if __name__ == "__main__":
    entrypoint()